# -*- coding: utf-8 -*-
"""Logistic Regression ML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jfyz4NM3kSaRo0xr8PosnNOhpoimpiVn

### Imports
"""

# Do NOT run unless coding in Colab
# Connect to Google Drive
# Make sure Auto.csv is save in /drive/MyDrive/Colab Notebooks/
from google.colab import drive
drive.mount('/drive', force_remount=True)

# Import libraries.
import numpy as np
import pandas as pd
import seaborn as sns
import statsmodels.api as sm
import statsmodels.formula.api as smf
from statsmodels.api import OLS
from sklearn.linear_model import LogisticRegression

"""### Load Data"""

# Load data file

# if importing data from Google Drive
Auto = pd.read_csv('/drive/MyDrive/Colab Notebooks/Auto.csv')

# otherwise
# Auto = pd.read_csv('Auto.csv')

"""### Check Data"""

# Check head.
Auto.head()

"""### Data Cleaning"""

# Save a copy of the dataset with the name column removed. We will not be using the ‘name’ variable.
Auto2 = Auto.drop('name', axis=1)

# Check to make sure column was dropped.
Auto2.head()

# Creat a copy of the dataframe for simple logistic regression with ML.
data = Auto2.copy()

"""### Log Reg # 1 (no one hot) Prep"""

# Commented out IPython magic to ensure Python compatibility.
# Import the necessary libraries for data processing and machine learning.
import numpy as np  # For numerical operations.
import pandas as pd  # For data manipulation.

# sklearn libraries for preprocessing, model selection, and metrics.
from sklearn.preprocessing import StandardScaler  # To standardize features.
from sklearn.linear_model import LogisticRegression  # Logistic regression model.
from sklearn.model_selection import train_test_split  # To split the data into training and test sets.
from sklearn.model_selection import KFold  # For K-fold cross-validation.
from sklearn.metrics import make_scorer, confusion_matrix  # For creating custom metrics and confusion matrices.
from sklearn.model_selection import learning_curve  # To generate a learning curve.

# Libraries for visualization.
import matplotlib.pyplot as plt  # For plotting.
import seaborn as sns  # For attractive and informative statistical graphics.
sns.set_style('whitegrid')  # Set the style of seaborn plots to 'whitegrid'.

# This command makes sure that plots are displayed inline in the Jupyter Notebook.
# %matplotlib inline

# Check data types.
data.dtypes

# Plot boxplot for each variable and check for outliers.
for column in data:
    plt.figure()
    data.boxplot([column])

# Perform capping for two columns with outliers.
# Calculate first and third quartile and IQR.

# Column = horsepower.
horsepowerQ1 = data['horsepower'].quantile(0.25)
horsepowerQ3 = data['horsepower'].quantile(0.75)
horsepower_IQR = horsepowerQ3 - horsepowerQ1

# Column = acceleration.
accelerationQ1 = data['acceleration'].quantile(0.25)
accelerationQ3 = data['acceleration'].quantile(0.75)
acceleration_IQR = accelerationQ3 - accelerationQ1

# Determine lower and upper bounds.

# Column = horsepower.
horsepower_low = horsepowerQ1 - 1.5 * horsepower_IQR
horsepower_upper = horsepowerQ3 + 1.5 * horsepower_IQR

# Column = acceleration.
acceleration_low = accelerationQ1 - 1.5 * acceleration_IQR
acceleration_upper = accelerationQ3 + 1.5 * acceleration_IQR

# Cap the values by replacing values below the lower bound with the lower bound values.
# And values above the upper bound with the upper bound value.
# Source: https://sparkbyexamples.com/pandas/pandas-replace-values-based-on-condition/

# Column = horsepower.
data['horsepower'] = np.where(data['horsepower'] > horsepower_upper, horsepower_upper, data['horsepower'])
data['horsepower'] = np.where(data['horsepower'] < horsepower_low, horsepower_low, data['horsepower'])

# Column = acceleration.
data['acceleration'] = np.where(data['acceleration'] > acceleration_upper, acceleration_upper, data['acceleration'])
data['acceleration'] = np.where(data['acceleration'] < acceleration_low, acceleration_low, data['acceleration'])

# Create a new binary variable named "HighMPG" where the value is 1 if mpg is above 27.5.
HighMPG = np.where(data['mpg'] > 27.5, 1, 0)

# Add column to dataframe.
data['HighMPG'] = HighMPG

# Drop previous mpg column.
del data['mpg']

# Check to make sure correct columns were added and removed.
data.head()

# Convert HighMPG column to a categorical variable.
data['HighMPG'] = data['HighMPG'].astype("category")

# Check that conversion was successful.
data.dtypes

# Split dataset into features and target.
X = data.drop(['HighMPG'],axis=1)
y = data['HighMPG']

# Set up the environment to ignore warnings.
import warnings
warnings.filterwarnings("ignore")

"""### USE THIS: Logistic Regression # 1 - No one hot encoding ✅
Summary Stats & Accuracy
"""

# Split dataset into training and test sets.
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.3)

# Scale the feature data.
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Add a constant term to the independent variables
# this is to avoid data leakage due to scaling feature data
# to allow for a baseline probability (intercept)
# this makes our model more flexible and interpretable
X_train_const = sm.add_constant(X_train)

# Fit logistic regression model
model = sm.Logit(y_train, X_train_const)
result = model.fit()

# Print the model summary and accuracy
print(result.summary(xname=['const', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'year', 'origin']))

# independent variables 1-7 are:
# 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'year', 'origin'

# Implement K-Fold cross-validation to evaluate the model's performance.
# This technique splits the data into 'k' consecutive folds, then for each fold, a model is trained on k-1 folds and validated on the remaining part.
# It provides a robust estimate of the model's performance on unseen data by averaging out the variability due to the particular random choice of train/test splits.
model = LogisticRegression()
scaler = StandardScaler()
kfold = KFold(n_splits=10)
kfold.get_n_splits(X)

# Initialize an array to hold the accuracy scores for each fold
accuracy = np.zeros(10)
np_idx = 0

# Perform the K-Fold cross-validation
for train_idx, test_idx in kfold.split(X):
    X_train, X_test = X.values[train_idx], X.values[test_idx]
    y_train, y_test = y.values[train_idx], y.values[test_idx]

    # Scale the features within the train/test split to prevent data leakage
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    # Train the model and make predictions
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)

    cm = confusion_matrix(y_test, predictions)

    if cm.shape == (1, 1):
      # If the confusion matrix has only one element, set TN, FP, FN, TP accordingly
      TN, FP, FN, TP = 0, 0, 0, cm[0, 0]

    else:
      # Compute the accuracy for the current fold
      TN = cm[0][0]
      FP = cm[0][1]
      FN = cm[1][0]
      TP = cm[1][1]

    total = float(TN + FP + FN + TP)
    ACC = (TP + TN) / float(total)

    # Store the accuracy in the array
    accuracy[np_idx] = ACC*100
    np_idx += 1

    # Print the accuracy for the current fold
    print("Fold {}: Accuracy: {}%".format(np_idx, round(ACC*100,3)))

# Print the average accuracy across all folds
print("Average Score for logistic regression without one-hot encoding: {}%({}%)".format(round(np.mean(accuracy),3),round(np.std(accuracy),3)))

"""### Feature Engineering"""

# Convert cylinders, year, and origin to factors as they are discrete variables.
Auto2['cylinders'] = Auto2['cylinders'].astype("category")
Auto2['year'] = Auto2['year'].astype("category")
Auto2['origin'] = Auto2['origin'].astype("category")

# Check data types to ensure conversion was successful.
Auto2.dtypes

# After one hot encoding, there will be 5 coefficients for cylinders, 13 for year, and 3 for origin.
# As such, we will group the years into decades to cut down on the number of coefficients.
# This will simplify result interpretation.

Auto2['year'].replace(71, 70, inplace=True)
Auto2['year'].replace(72, 70, inplace=True)
Auto2['year'].replace(73, 70, inplace=True)
Auto2['year'].replace(74, 70, inplace=True)
Auto2['year'].replace(75, 70, inplace=True)
Auto2['year'].replace(76, 70, inplace=True)
Auto2['year'].replace(77, 70, inplace=True)
Auto2['year'].replace(78, 70, inplace=True)
Auto2['year'].replace(79, 70, inplace=True)
Auto2['year'].replace(81, 80, inplace=True)
Auto2['year'].replace(82, 80, inplace=True)

# Check to make sure conversion was successful.
Auto2.head(10)

# One hot encode categorical variables.
# Create a list of categorical features.
# Iterate through the list and one hot encode them.
# This will help to represent categorical variables as numerical values in the regression models.

# Create list of categorical features.
categorical_features = ['cylinders', 'year', 'origin']

# One hot encode.
for feature in categorical_features:
    onehot = pd.get_dummies(Auto2[feature], prefix=feature)
    Auto2 = Auto2.drop(feature, axis=1)
    Auto2 = Auto2.join(onehot)

# Create a new binary variable named "HighMPG" where the value is 1 if mpg is above 27.5.
HighMPG = np.where(Auto2['mpg'] > 27.5, 1, 0)

# Add column to dataframe.
Auto2['HighMPG'] = HighMPG

# Drop previous mpg column.
del Auto2['mpg']

# Check to make sure correct columns were added and removed.
Auto2.head()

Auto2.describe()

"""## USE THIS: Logistic Regression # 2 - With one hot encoding ✅
Accuracy

### Logistic Regssion (with one hot) without constant term
"""

# Check data types.
Auto2.dtypes

# Perform capping for two columns with outliers.
# Calculate first and third quartile and IQR.

# Column = horsepower.
horsepowerQ1 = Auto2['horsepower'].quantile(0.25)
horsepowerQ3 = Auto2['horsepower'].quantile(0.75)
horsepower_IQR = horsepowerQ3 - horsepowerQ1

# Column = acceleration.
accelerationQ1 = Auto2['acceleration'].quantile(0.25)
accelerationQ3 = Auto2['acceleration'].quantile(0.75)
acceleration_IQR = accelerationQ3 - accelerationQ1

# Determine lower and upper bounds.

# Column = horsepower.
horsepower_low = horsepowerQ1 - 1.5 * horsepower_IQR
horsepower_upper = horsepowerQ3 + 1.5 * horsepower_IQR

# Column = acceleration.
acceleration_low = accelerationQ1 - 1.5 * acceleration_IQR
acceleration_upper = accelerationQ3 + 1.5 * acceleration_IQR

# Cap the values by replacing values below the lower bound with the lower bound values.
# And values above the upper bound with the upper bound value.
# Source: https://sparkbyexamples.com/pandas/pandas-replace-values-based-on-condition/

# Column = horsepower.
Auto2['horsepower'] = np.where(Auto2['horsepower'] > horsepower_upper, horsepower_upper, Auto2['horsepower'])
Auto2['horsepower'] = np.where(Auto2['horsepower'] < horsepower_low, horsepower_low, Auto2['horsepower'])

# Column = acceleration.
Auto2['acceleration'] = np.where(Auto2['acceleration'] > acceleration_upper, acceleration_upper, Auto2['acceleration'])
Auto2['acceleration'] = np.where(Auto2['acceleration'] < acceleration_low, acceleration_low, Auto2['acceleration'])

# Convert HighMPG column to a categorical variable.
Auto2['HighMPG'] = Auto2['HighMPG'].astype("category")

# Check that conversion was successful.
Auto2.dtypes

# Split dataset into features and target.
X = Auto2.drop(['HighMPG','cylinders_4','year_70','origin_1'],axis=1)
y = Auto2['HighMPG']

# Split dataset into training and test sets.
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.3)

# Scale the feature data.
scaler = StandardScaler()
# only scale the non-hot encoded columns
col_to_scale = ['displacement','horsepower','weight','acceleration']

X_train = np.concatenate((scaler.fit_transform(X_train[col_to_scale]),X_train.iloc[:, 4:].values), axis = 1)
X_test = np.concatenate((scaler.fit_transform(X_test[col_to_scale]),X_test.iloc[:, 4:].values), axis = 1)

# Set up the environment to ignore warnings.
import warnings
warnings.filterwarnings("ignore")

# Create a logistic regression model and train it with the training data.
model = sm.Logit(y_train, X_train)
result = model.fit()

# Print the model summary and accuracy
print(result.summary(xname=X.columns.tolist()))

# Implement K-Fold cross-validation to evaluate the model's performance.
# This technique splits the data into 'k' consecutive folds, then for each fold, a model is trained on k-1 folds and validated on the remaining part.
# It provides a robust estimate of the model's performance on unseen data by averaging out the variability due to the particular random choice of train/test splits.
model = LogisticRegression()
scaler = StandardScaler()
kfold = KFold(n_splits=10)
kfold.get_n_splits(X)

# Initialize an array to hold the accuracy scores for each fold
accuracy = np.zeros(10)
np_idx = 0

# Perform the K-Fold cross-validation
for train_idx, test_idx in kfold.split(X):
    X_train, X_test = X.values[train_idx], X.values[test_idx]
    y_train, y_test = y.values[train_idx], y.values[test_idx]

    # Scale the features within the train/test split to prevent data leakage
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    # Train the model and make predictions
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)

    cm = confusion_matrix(y_test, predictions)

    if cm.shape == (1, 1):
      # If the confusion matrix has only one element, set TN, FP, FN, TP accordingly
      TN, FP, FN, TP = 0, 0, 0, cm[0, 0]

    else:
      # Compute the accuracy for the current fold
      TN = cm[0][0]
      FP = cm[0][1]
      FN = cm[1][0]
      TP = cm[1][1]

    total = float(TN + FP + FN + TP)
    ACC = (TP + TN) / float(total)

    # Store the accuracy in the array
    accuracy[np_idx] = ACC*100
    np_idx += 1

    # Print the accuracy for the current fold
    print("Fold {}: Accuracy: {}%".format(np_idx, round(ACC*100,3)))

# Print the average accuracy across all folds
print("Average Score for logistic regression with one-hot encoding: {}%({}%)".format(round(np.mean(accuracy),3),round(np.std(accuracy),3)))

"""### Logistic Regression (with one hot) with constant term

"""

# Split dataset into features and target.
X = Auto2.drop(['HighMPG','cylinders_3','year_70','origin_1'],axis=1)
y = Auto2['HighMPG']

# Split dataset into training and test sets.
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.3)

# Scale the feature data.
scaler = StandardScaler()
# only scale the non-hot encoded columns
col_to_scale = ['displacement','horsepower','weight','acceleration']

X_train = np.concatenate((scaler.fit_transform(X_train[col_to_scale]),X_train.iloc[:, 4:].values), axis = 1)
X_test = np.concatenate((scaler.fit_transform(X_test[col_to_scale]),X_test.iloc[:, 4:].values), axis = 1)

# Set up the environment to ignore warnings.
import warnings
warnings.filterwarnings("ignore")

# Add a constant term to the independent variables
# this is to avoid data leakage due to scaling feature data
# to allow for a baseline probability (intercept)
# this makes our model more flexible and interpretable
X_train_const = sm.add_constant(X_train)

X_train_const

y_train

# Create a logistic regression model and train it with the training data.
model = sm.Logit(y_train, X_train_const)
result = model.fit()

X.columns.tolist()

# Print the model summary and accuracy
# xname=X.columns.tolist()
print(result.summary(xname=['const',
                            'displacement','horsepower','weight','acceleration',
                            'cylinders_4','cylinders_5','cylinders_6','cylinders_8',
                            'year_80',
                            'origin_2','origin_3']))

# Implement K-Fold cross-validation to evaluate the model's performance.
# This technique splits the data into 'k' consecutive folds, then for each fold, a model is trained on k-1 folds and validated on the remaining part.
# It provides a robust estimate of the model's performance on unseen data by averaging out the variability due to the particular random choice of train/test splits.
model = LogisticRegression()
scaler = StandardScaler()
kfold = KFold(n_splits=10)
kfold.get_n_splits(X)

# Initialize an array to hold the accuracy scores for each fold
accuracy = np.zeros(10)
np_idx = 0

# Perform the K-Fold cross-validation
for train_idx, test_idx in kfold.split(X):
    X_train, X_test = X.values[train_idx], X.values[test_idx]
    y_train, y_test = y.values[train_idx], y.values[test_idx]

    # Scale the features within the train/test split to prevent data leakage
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    # Train the model and make predictions
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)

    cm = confusion_matrix(y_test, predictions)

    if cm.shape == (1, 1):
      # If the confusion matrix has only one element, set TN, FP, FN, TP accordingly
      TN, FP, FN, TP = 0, 0, 0, cm[0, 0]

    else:
      # Compute the accuracy for the current fold
      TN = cm[0][0]
      FP = cm[0][1]
      FN = cm[1][0]
      TP = cm[1][1]

    total = float(TN + FP + FN + TP)
    ACC = (TP + TN) / float(total)

    # Store the accuracy in the array
    accuracy[np_idx] = ACC*100
    np_idx += 1

    # Print the accuracy for the current fold
    print("Fold {}: Accuracy: {}%".format(np_idx, round(ACC*100,3)))

# Print the average accuracy across all folds
print("Average Score for logistic regression with one-hot encoding: {}%({}%)".format(round(np.mean(accuracy),3),round(np.std(accuracy),3)))