# -*- coding: utf-8 -*-
"""ClusteringMultinomialLogisticRegression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1M4FYE7YcOJce70_4GdLtsJWa3ibPDo1A
"""

# Do NOT run unless coding in Colab
# Connect to Google Drive
# Make sure Auto.csv is save in /drive/MyDrive/Colab Notebooks/
from google.colab import drive
drive.mount('/drive', force_remount=True)

# Import libraries.
import numpy as np
import pandas as pd
import seaborn as sns
import statsmodels.api as sm
import statsmodels.formula.api as smf
from statsmodels.api import OLS
from sklearn.linear_model import LogisticRegression

# Load data file

# if importing data from Google Drive
Auto = pd.read_csv('/drive/MyDrive/Colab Notebooks/Auto.csv')

# otherwise
# Auto = pd.read_csv('Auto.csv')

# Check head.
Auto.head()

# Save a copy of the dataset with the name column removed. We will not be using the ‘name’ variable.
Auto2 = Auto.drop('name', axis=1)

# Check to make sure column was dropped.
Auto2.head()

# Creat a copy of the dataframe for simple logistic regression with ML.
data = Auto2.copy()

"""NO ONEHOT ENCODING"""

# Commented out IPython magic to ensure Python compatibility.
# Import the necessary libraries for data processing and machine learning.
import numpy as np  # For numerical operations.
import pandas as pd  # For data manipulation.

# sklearn libraries for preprocessing, model selection, and metrics.
from sklearn.preprocessing import StandardScaler  # To standardize features.
from sklearn.linear_model import LogisticRegression  # Logistic regression model.
from sklearn.model_selection import train_test_split  # To split the data into training and test sets.
from sklearn.model_selection import KFold  # For K-fold cross-validation.
from sklearn.metrics import make_scorer, confusion_matrix  # For creating custom metrics and confusion matrices.
from sklearn.model_selection import learning_curve  # To generate a learning curve.

# Libraries for visualization.
import matplotlib.pyplot as plt  # For plotting.
import seaborn as sns  # For attractive and informative statistical graphics.
sns.set_style('whitegrid')  # Set the style of seaborn plots to 'whitegrid'.

# This command makes sure that plots are displayed inline in the Jupyter Notebook.
# %matplotlib inline

# Check data types.
data.dtypes

# Plot boxplot for each variable and check for outliers.
for column in data:
    plt.figure()
    data.boxplot([column])

# Perform capping for two columns with outliers.
# Calculate first and third quartile and IQR.

# Column = horsepower.
horsepowerQ1 = data['horsepower'].quantile(0.25)
horsepowerQ3 = data['horsepower'].quantile(0.75)
horsepower_IQR = horsepowerQ3 - horsepowerQ1

# Column = acceleration.
accelerationQ1 = data['acceleration'].quantile(0.25)
accelerationQ3 = data['acceleration'].quantile(0.75)
acceleration_IQR = accelerationQ3 - accelerationQ1

# Determine lower and upper bounds.

# Column = horsepower.
horsepower_low = horsepowerQ1 - 1.5 * horsepower_IQR
horsepower_upper = horsepowerQ3 + 1.5 * horsepower_IQR

# Column = acceleration.
acceleration_low = accelerationQ1 - 1.5 * acceleration_IQR
acceleration_upper = accelerationQ3 + 1.5 * acceleration_IQR

# Cap the values by replacing values below the lower bound with the lower bound values.
# And values above the upper bound with the upper bound value.
# Source: https://sparkbyexamples.com/pandas/pandas-replace-values-based-on-condition/

# Column = horsepower.
data['horsepower'] = np.where(data['horsepower'] > horsepower_upper, horsepower_upper, data['horsepower'])
data['horsepower'] = np.where(data['horsepower'] < horsepower_low, horsepower_low, data['horsepower'])

# Column = acceleration.
data['acceleration'] = np.where(data['acceleration'] > acceleration_upper, acceleration_upper, data['acceleration'])
data['acceleration'] = np.where(data['acceleration'] < acceleration_low, acceleration_low, data['acceleration'])

# Elbow method
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

data['mpg'] = pd.to_numeric(data['mpg'], errors='coerce')

wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=25, random_state=0)
    kmeans.fit(data[['mpg']])
    wcss.append(kmeans.inertia_)

plt.plot(range(1, 11), wcss, marker='o')
plt.title('Elbow Method')
plt.xlabel('Number of Clusters')
plt.ylabel('Within groups sum of squares')
plt.show()

from sklearn.cluster import KMeans
import numpy as np
np.random.seed(123)

# k-means
kmeans = KMeans(n_clusters=3, init='k-means++', max_iter=300, n_init=25, random_state=0)
data['mpg_cluster'] = kmeans.fit_predict(data[['mpg']])
data

#visualizing clusters
sns.scatterplot(x='mpg_cluster', y='mpg', data=data)
plt.title('MPG vs MPG Cluster')
plt.xlabel('MPG Cluster')
plt.ylabel('MPG')
plt.show()
# as seen here, the k-means clustering has worked and we see a clear distinction between the mpg groupings, and it is interesting to note that the
# model used 1 as the lowest numerical value group and 0 as the next lowest.

data = data.drop(columns=['mpg']) #feature engineering

import statsmodels.api as sm
from statsmodels.discrete.discrete_model import MNLogit
from sklearn.model_selection import train_test_split

# Load independent and dependent variables.
X = data.drop(['mpg_cluster'], axis=1)
y = data['mpg_cluster']

X_const = sm.add_constant(X)

X_train, X_test, y_train, y_test = train_test_split(X_const, y, test_size=0.3, random_state=42)

# multinomial logistic regression
model = MNLogit(y_train, X_train)
result = model.fit()

#PREDICTIONS
predictions = result.predict(X_test).idxmax(axis=1)

cm = confusion_matrix(y_test, predictions, labels=[0, 1, 2])
TN = cm[0][0]
FP = cm[0][1:].sum()
FN = cm[1:, 0].sum()
TP = cm[1:, 1:].sum()

# Calculate the total number of cases and the accuracy
total = TN + FP + FN + TP
ACC = (TP + TN) / float(total)

# Print the model summary and accuracy
print(result.summary())
print("This model got an accuracy of {}% on the testing set".format(round(ACC*100, 2)))

# Convert cylinders, year, and origin to factors as they are discrete variables.
Auto2['cylinders'] = Auto2['cylinders'].astype("category")
Auto2['year'] = Auto2['year'].astype("category")
Auto2['origin'] = Auto2['origin'].astype("category")

# Check data types to ensure conversion was successful.
Auto2.dtypes

# After one hot encoding, there will be 5 coefficients for cylinders, 13 for year, and 3 for origin.
# As such, we will group the years into decades to cut down on the number of coefficients.
# This will simplify result interpretation.

Auto2['year'].replace(71, 70, inplace=True)
Auto2['year'].replace(72, 70, inplace=True)
Auto2['year'].replace(73, 70, inplace=True)
Auto2['year'].replace(74, 70, inplace=True)
Auto2['year'].replace(75, 70, inplace=True)
Auto2['year'].replace(76, 70, inplace=True)
Auto2['year'].replace(77, 70, inplace=True)
Auto2['year'].replace(78, 70, inplace=True)
Auto2['year'].replace(79, 70, inplace=True)
Auto2['year'].replace(81, 80, inplace=True)
Auto2['year'].replace(82, 80, inplace=True)

# Check to make sure conversion was successful.
Auto2.head(10)

# One hot encode categorical variables.
# Create a list of categorical features.
# Iterate through the list and one hot encode them.
# This will help to represent categorical variables as numerical values in the regression models.

# Create list of categorical features.
categorical_features = ['cylinders', 'year', 'origin']

# One hot encode.
for feature in categorical_features:
    onehot = pd.get_dummies(Auto2[feature], prefix=feature)
    Auto2 = Auto2.drop(feature, axis=1)
    Auto2 = Auto2.join(onehot)

# Check data types.
Auto2.dtypes

# Perform capping for two columns with outliers.
# Calculate first and third quartile and IQR.

# Column = horsepower.
horsepowerQ1 = Auto2['horsepower'].quantile(0.25)
horsepowerQ3 = Auto2['horsepower'].quantile(0.75)
horsepower_IQR = horsepowerQ3 - horsepowerQ1

# Column = acceleration.
accelerationQ1 = Auto2['acceleration'].quantile(0.25)
accelerationQ3 = Auto2['acceleration'].quantile(0.75)
acceleration_IQR = accelerationQ3 - accelerationQ1

# Determine lower and upper bounds.

# Column = horsepower.
horsepower_low = horsepowerQ1 - 1.5 * horsepower_IQR
horsepower_upper = horsepowerQ3 + 1.5 * horsepower_IQR

# Column = acceleration.
acceleration_low = accelerationQ1 - 1.5 * acceleration_IQR
acceleration_upper = accelerationQ3 + 1.5 * acceleration_IQR

# Cap the values by replacing values below the lower bound with the lower bound values.
# And values above the upper bound with the upper bound value.
# Source: https://sparkbyexamples.com/pandas/pandas-replace-values-based-on-condition/

# Column = horsepower.
Auto2['horsepower'] = np.where(Auto2['horsepower'] > horsepower_upper, horsepower_upper, Auto2['horsepower'])
Auto2['horsepower'] = np.where(Auto2['horsepower'] < horsepower_low, horsepower_low, Auto2['horsepower'])

# Column = acceleration.
Auto2['acceleration'] = np.where(Auto2['acceleration'] > acceleration_upper, acceleration_upper, Auto2['acceleration'])
Auto2['acceleration'] = np.where(Auto2['acceleration'] < acceleration_low, acceleration_low, Auto2['acceleration'])

# Elbow method
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

Auto2['mpg'] = pd.to_numeric(Auto2['mpg'], errors='coerce')

wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=25, random_state=0)
    kmeans.fit(Auto2[['mpg']])
    wcss.append(kmeans.inertia_)

plt.plot(range(1, 11), wcss, marker='o')
plt.title('Elbow Method')
plt.xlabel('Number of Clusters')
plt.ylabel('Within groups sum of squares')
plt.show()

"""From the plot, we can see after 3 clusters,  the within groups sum of squares do not significantly decrease after increasing the number of clusters.

"""

from sklearn.cluster import KMeans
import numpy as np
np.random.seed(123)

# k-means
kmeans = KMeans(n_clusters=3, init='k-means++', max_iter=300, n_init=25, random_state=0)
Auto2['mpg_cluster'] = kmeans.fit_predict(Auto2[['mpg']])

Auto2 = Auto2.drop(columns=['mpg']) #feature engineering

Auto2

# Convert HighMPG column to a categorical variable.
Auto2['mpg_cluster'] = Auto2['mpg_cluster'].astype("category")

# Check that conversion was successful.
Auto2.dtypes

# Load independent and dependent variables.
X = Auto2.drop(['mpg_cluster'], axis=1)
y = Auto2['mpg_cluster']

X_const = sm.add_constant(X)

X_train, X_test, y_train, y_test = train_test_split(X_const, y, test_size=0.3, random_state=42)

model = MNLogit(y_train, X_train)
result = model.fit()

#PREDICTIONS
predictions = result.predict(X_test).idxmax(axis=1)

cm = confusion_matrix(y_test, predictions, labels=[0, 1, 2])
TN = cm[0][0]
FP = cm[0][1:].sum()
FN = cm[1:, 0].sum()
TP = cm[1:, 1:].sum()

# Calculate the total number of cases and the accuracy
total = TN + FP + FN + TP
ACC = (TP + TN) / float(total)

# Print the model summary and accuracy
print(result.summary())
print("This model got an accuracy of {}% on the testing set".format(round(ACC*100, 2)))